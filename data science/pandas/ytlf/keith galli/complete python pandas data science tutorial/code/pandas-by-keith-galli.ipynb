{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python Pandas Learn - Keith Galli"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this video \n",
    "[Complete Python Pandas Data Science Tutorial! (Reading CSV/Excel files, Sorting, Filtering, Groupby)](https://www.youtube.com/watch?v=vmEHCJofslg&t=13s&ab_channel=KeithGalli) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "install commands \n",
    "\n",
    "```shell\n",
    "pip install pandas\n",
    "pip install openpyxl\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vs code jupyter notebook hotkeys\n",
    "\n",
    "-- for markdown blocks --\n",
    "```\n",
    "ENTER : next line / start editing\n",
    "CTRL + ALT + ENTER : stop editing\n",
    "```\n",
    "\n",
    "-- for python blocks --\n",
    "\n",
    "```\n",
    "CTRL + ALT + ENTER : execute the code block\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ðŸ”´ incase you mess up the data , always keep a backup of the original \n",
    "#ðŸ”´ you can also create various stages of your data as it goes through various processings\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('../dataset/pokemon_data.csv')\n",
    "# df_xlsx = pd.read_excel('../dataset/pokemon_data.xlsx') # required me to install openpyxl manually \n",
    "# df_txt = pd.read_csv('../dataset/pokemon_data.txt',delimiter='\\t') # its forward slash"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read top three rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df.head(3))\n",
    "# print (df_xlsx.head(3)) \n",
    "# print(df_txt.head(3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read bottom three rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df.tail(3))\n",
    "# print(df_xlsx.tail(3))\n",
    "# print(df_txt.tail(3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reading data in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read headers\n",
    "print(df.columns)\n",
    "\n",
    "## reach each column\n",
    "# print(df['Name'].head(5)) # top 5 \n",
    "# print(df['Name'][0:3]) # get 0 to 3 , skips 3\n",
    "# print(df['Name'][4:7]) # get 4 to 7 , skips 7 \n",
    "# print(df.Name[0:5]) # does not work for table headers less than two words\n",
    "\n",
    "## multiple column headers\n",
    "# print(df[['Name','Type 1','Type 2']].head(5))\n",
    "# print(df[['Name','Type 1','Type 2']].tail(5))\n",
    "# print(df[['Name','Type 1','Type 2','HP']] [0:5])\n",
    "\n",
    "## read each row \n",
    "# iloc mean integer location\n",
    "# print(df.iloc[77]) # this prints record on index of 77 \n",
    "# print(df.iloc[100]) # this prints record on index of 100 \n",
    "# print(df.iloc[100:104]) # multiple rows , 100 to 103 , skips 104\n",
    "\n",
    "\n",
    "## read a specific location (row,column) , like a cell data in excel\n",
    "# print(df['Name'].head(3)) # top 5 \n",
    "# print(df.iloc[2,1]) # outputs Venusaur as its on second index , first column\n",
    "\n",
    "## 13:11 iterate through each row as I am reading it \n",
    "# for index , row in df.iterrows():\n",
    "    # print(index,row)\n",
    "    # print(index,row['Name'])\n",
    "    # print(index,row[['Name','HP']]) # works but not the cleanest output\n",
    "\n",
    "# combine loc with iterrows to further narrow down the iterable data\n",
    "# for index , row in df.loc[df['Type 1'] == 'Fire'].iterrows():\n",
    "#     # print(index,row)\n",
    "#     print(index,row['Name'])\n",
    "\n",
    "## 14:15 querying like sql where clause \n",
    "# print(df.loc[df['Type 1'] == 'Fire'])\n",
    "# print(df.loc[df['Type 1'] == 'Grass'])\n",
    "# print(df[['Name','Type 1','Type 2']].loc[df['Type 1'] == 'Fire']) # specific columns , like select column in sql\n",
    "# print(df.loc[df['Type 1'] == \"Fire\"].head(5)) # applying head limits to top 5\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sorting or describing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 15:52 describe gives count, mean, std deviation, min, max type stats \n",
    "# df.describe()\n",
    "\n",
    "## sorting\n",
    "# df.sort_values('Name').head(5)\n",
    "# df.sort_values('Name',ascending=False).head(5) # descending\n",
    "# df.sort_values('Name',ascending=False).head(5) # descending\n",
    "# df.sort_values(['Type 1','HP']).head(5) # sort by multiple columns \n",
    "# df.sort_values(['Type 1','HP'],ascending=[1,0]).head(5) # sort by multiple columns , ascending param different for each column"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "making changes to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18:22\n",
    "# adding a new total column to the df which would sum up multiple columns \n",
    "# new total column = hp + attack + defense + sp attack + sp def + speed \n",
    "# df['Total'] = df['HP'] + df['Attack'] + df['Defense'] + df['Sp. Atk'] + df['Sp. Def'] + df['Speed']\n",
    "df.head(3)\n",
    "## verify the actual total for eg bulbasaur here \n",
    "# 45+49+49+65+65+45 # 318\n",
    "\n",
    "## alternate way to add columns using iloc\n",
    "# axis = 1 add horizhontally , axis = 0 add vertically \n",
    "# to include 9th col you need to go till the 10th column \n",
    "df['Total'] = df.iloc[:, 4:10].sum(axis=1) # 4th col hp to 9th col speed\n",
    "df.head(3)\n",
    "\n",
    "## using numbers like 4 till 10 is not the best way\n",
    "## use the index of hp and then goto the index of speed \n",
    "\n",
    "## drop column \n",
    "# df = df.drop(columns=['Total'])\n",
    "# df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 24.20 placing the total column where we want it to be\n",
    "## it makes more sense that it resides to the left of hp \n",
    "\n",
    "# df = df[['Total','HP','Defense']] \n",
    "\n",
    "## because its annoying to type all the columns \n",
    "## we can get the cols as a list \n",
    "\n",
    "# cols = list(df.columns.values)\n",
    "# df = df[cols[0:4] + [cols[-1]] + cols[4:12]] # moves total after type 2 column \n",
    "# df.head(5)\n",
    "\n",
    "## protip: dont rely on numbers much as if your data is changing \n",
    "## and if the columns in the file change then things will mess up\n",
    "## its best to use names and it clearly defines what you want to work with\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "saving the data (export to desired format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 28.13 \n",
    "df.head(3)\n",
    "\n",
    "## save to csv\n",
    "# df.to_csv('../dataset/code_modified_pokemon_data.csv')\n",
    "\n",
    "# remove the additional index column added at the start \n",
    "# df.to_csv('../dataset/code_modified_pokemon_data.csv',index=False) \n",
    "\n",
    "## there is no delmiter param but rather a sep for seperator \n",
    "# df.to_csv('../dataset/code_modified_pokemon_data.txt',index=False, sep='\\t') \n",
    "\n",
    "## to excel\n",
    "# df.to_excel('../dataset/code_modified_pokemon_data.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filtering data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## adding multiple lines \n",
    "# Add \"<SPACE>\\\" at the end of the first line before hitting <ENTER>.\n",
    "\n",
    "# df.loc[df['Type 1'] == 'Grass']\n",
    "# df.loc[df['Type 1'] != 'Grass'].head(5) # not equals \n",
    "# df.loc[(df['Type 1'] == 'Grass') & (df['Type 2'] == 'Poison')] # lets add one more clause\n",
    "# df.loc[(df['Type 1'] == 'Grass') | (df['Type 2'] == 'Poison')] # or condition\n",
    "\n",
    "## select specific columns\n",
    "# new_df = df[['Name','Type 1','Type 2','HP']] \\\n",
    "        # .loc[(df['Type 1'] == 'Grass') & (df['Type 2'] == 'Poison') & (df['HP'] > 70 )] \n",
    "\n",
    "# new_df # this still shows the old index \n",
    "# to reset the index we can do this \n",
    "# new_df = new_df.reset_index() # it saves old index as new column\n",
    "# new_df = new_df.reset_index(drop=True) # if you dont want old index to be saved\n",
    "# new_df.reset_index(drop=True,inplace=True) # if you dont want re assign to new_df again , you can do it in place\n",
    "\n",
    "# new_df  \n",
    "\n",
    "## like operator, contains operation\n",
    "# filter mega versions of pokemons , they keyword have Mega in their name \n",
    "# df.loc[df['Name'].str.contains('Mega')].count()\n",
    "# df['Name'].loc[df['Name'].str.contains('Mega')].head(5)\n",
    "\n",
    "## not operator ~ \n",
    "# df['Name'].loc[~df['Name'].str.contains('Mega')].head(5)\n",
    "\n",
    "## contains is very powerful here as we can pass in regex to it\n",
    "\n",
    "# cols = list(df.columns.values)\n",
    "# df[cols[0:4]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regex filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## regex filtering\n",
    "import re\n",
    "\n",
    "## contains x or y \n",
    "# df[['Name','Type 1','Type 2','HP']] \\\n",
    "#     .loc[df['Type 1'].str.contains('fire|grass',regex=True,flags=re.I)].head(5) # re.I can be used to ignore case\n",
    "\n",
    "# df[['Name','Type 1','Type 2','HP']] \\\n",
    "#     .loc[df['Type 1'].str.contains('Fire|Grass',regex=True)].head(5) # case is important\n",
    "\n",
    "## starts with\n",
    "## get all pokemon names that started with a \"pi\"\n",
    "df[['Name','Type 1','Type 2','HP']] \\\n",
    "    .loc[df['Name'].str.contains('^pi[a-z]*',regex=True,flags=re.I)].head(5)\n",
    "\n",
    "## phrase in middle 'geo'\n",
    "# df[['Name','Type 1','Type 2','HP']] \\\n",
    "#     .loc[df['Name'].str.contains('geo[a-z]*',regex=True,flags=re.I)].head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conditional changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.loc[df['Type 1'] == 'Fire','Type 1'] = 'Flamer'\n",
    "# df.loc[df['Type 1'] == 'Flamer'].head(5)\n",
    "\n",
    "## revert\n",
    "# df.loc[df['Type 1'] == 'Flamer','Type 1'] = 'Fire'\n",
    "# df.loc[df['Type 1'] == 'Fire'].head(5)\n",
    "\n",
    "## ðŸ”´ This is destructive , no way to revert the dataframe than to reset the whole notebook\n",
    "## make all legendary pokemon of type fire \n",
    "# df.loc[df['Type 1'] == 'Fire', 'Legendary'] = True # makes legendary column true for all fire pokemons\n",
    "# df[['Name','Type 1','Legendary']].loc[df['Type 1'] == 'Fire'].head(5)\n",
    "\n",
    "## change multiple params at a time\n",
    "# https://youtu.be/vmEHCJofslg?t=2755 \n",
    "\n",
    "# df.loc[df['Total'] > 500,['Generation','Legendary']] = 'TEST VALUE'\n",
    "# df\n",
    "\n",
    "## modify individually instead of set as one value to TEST VALUE\n",
    "# df.loc[df['Total'] > 500,['Generation','Legendary']] = ['Test 1','Test 2']\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aggregate statistics - group by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you got a similar TypeError after a groupby operation (e.g. TypeError: Could not convert ace to numeric), \n",
    "# then you probably have pandas>=2.0.\n",
    "# groupby.mean() has numeric_only= argument whose default value was True in the past but since pandas 2.0, \n",
    "# its default value is False. An implication is that string columns are not dropped when a statistical method \n",
    "# such as mean or std is called on the groupby object (as was done in the past). To solve the issue, pass numeric_only=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to pass numeric_only=True to mean function since pandas 2.0 \n",
    "# https://stackoverflow.com/questions/44522741/pandas-mean-typeerror-could-not-convert-to-numeric\n",
    "\n",
    "# df.groupby(['Type 1']).mean(numeric_only=True)\n",
    "# df.groupby(['Type 1']).mean(numeric_only=True).sort_values('Defense',ascending=False)\n",
    "# df.groupby(['Type 1']).mean(numeric_only=True).sort_values('Attack',ascending=False)\n",
    "# df.groupby(['Type 1']).mean(numeric_only=True).sort_values('HP',ascending=False)\n",
    "# df.groupby(['Type 1']).mean(numeric_only=True).sort_values('Speed',ascending=False)\n",
    "\n",
    "# aggregate statistics can be done with mean/sum/count\n",
    "# df.groupby(['Type 1']).sum(numeric_only=True)\n",
    "# df.groupby(['Type 1']).count().sort_values('Type 1',ascending=False)\n",
    "\n",
    "## clean count output\n",
    "# df['count'] = 1 # added a count column\n",
    "# df.groupby(['Type 1']).count()['count'] # set the values in the count column\n",
    "# df.groupby(['Type 1','Type 2']).count()['count'] # multiple group by\n",
    "\n",
    "# df = df.drop(columns=['count']) # drop the added column after work is done \n",
    "# df.head() # check if the column is dropped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "working with large amounts of data like 20gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python allows loading of such large 20 gb files\n",
    "# it allows you to read chunks at a time \n",
    "# you need to do some calculation on chunksize you might need\n",
    "# here chunksize 5 means 5 rows, each row be like 10 or 20 kb of data for this example\n",
    "# you need to calculate how many rows of data your ram can handle\n",
    "\n",
    "# for df in pd.read_csv('../dataset/pokemon_data.csv', chunksize=5):\n",
    "#     print('Chunk DF')\n",
    "#     print(df)\n",
    "\n",
    "# perform aggregate count using the chunk approach\n",
    "# temp_df = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "# for df in pd.read_csv('../dataset/pokemon_data.csv', chunksize=5):\n",
    "#     results = df.groupby(['Type 1']).count()\n",
    "#     temp_df = pd.concat([temp_df,results]) \n",
    "\n",
    "# print(temp_df)\n",
    "\n",
    "# this above is not a working example but the logic is kind of \n",
    "# like that and correct code needs to be figured.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
